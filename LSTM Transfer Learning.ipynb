{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adceab28-8144-40e4-a054-38f843cfed88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import *\n",
    "from tqdm import tqdm\n",
    "from utils import Load_Rumours_Dataset_filtering_since_first_post_Transfer_Learning\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import xgboost as xgb\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "459baa4a-bebe-4d5e-b4d8-703538b97f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rumour\n",
      "0    69\n",
      "1    59\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_dataset = 'charlie_hebdo'\n",
    "test_dataset = 'sydneysiege'\n",
    "time_cut =130\n",
    "processor = Load_Rumours_Dataset_filtering_since_first_post_Transfer_Learning(train_dataset,\\\n",
    "           test_dataset, time_cut=time_cut,test_size=0.7)\n",
    "\n",
    "processor.load_data()\n",
    "processor.process_data()\n",
    "train,test = processor.get_final_dataframes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "551f7876-9528-433d-88db-019436584338",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train  = train.drop(columns=['rumour'])\n",
    "X_train = np.hstack([X_train.drop(columns=['embeddings_avg']).values, np.array(pd.DataFrame(X_train.embeddings_avg.tolist()))])\n",
    "\n",
    "\n",
    "X_test  = test.drop(columns=['rumour'])\n",
    "X_test = np.hstack([X_test.drop(columns=['embeddings_avg']).values, np.array(pd.DataFrame(X_test.embeddings_avg.tolist()))])\n",
    "\n",
    "#X = np.hstack([X.drop(columns=['embeddings_avg']).values, np.array(pd.DataFrame(X.embeddings_avg.tolist()))])\n",
    "y_train =train['rumour']\n",
    "y_test =test['rumour']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7c473bd-3fe7-4cc7-bb9d-de8f1f2b4748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SMOTE to the features without embeddings\n",
    "smote = SMOTE(random_state=42,sampling_strategy='minority')\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4d6eced-29ad-4a84-928b-18db0f0b6eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "\n",
    "# Model definition\n",
    "class RumorDetectionLSTM(nn.Module):\n",
    "    def __init__(self, embedding_dim=100, lstm_hidden_size=32, dense_hidden_size=16):\n",
    "        super(RumorDetectionLSTM, self).__init__()\n",
    "        \n",
    "        # LSTM for the 100-dimensional embeddings\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=lstm_hidden_size, batch_first=True)\n",
    "        \n",
    "        # Dense layers for other features\n",
    "        self.dense1 = nn.Linear(8, 16)  # 8 non-embedding features\n",
    "        self.dense2 = nn.Linear(16, dense_hidden_size)\n",
    "        \n",
    "        # Combine LSTM and dense features\n",
    "        self.fc1 = nn.Linear(lstm_hidden_size + dense_hidden_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Separate embeddings and other features\n",
    "        embeddings = x[:, -100:].unsqueeze(1)  # (batch, seq_len=1, embedding_dim)\n",
    "        other_features = x[:, :8]  # First 8 features\n",
    "        \n",
    "        # LSTM output\n",
    "        lstm_out, _ = self.lstm(embeddings)\n",
    "        lstm_out = lstm_out[:, -1, :]  # Get the last LSTM output\n",
    "        \n",
    "        # Dense layers for other features\n",
    "        dense_out = torch.relu(self.dense1(other_features))\n",
    "        dense_out = torch.relu(self.dense2(dense_out))\n",
    "        \n",
    "        # Concatenate LSTM and dense outputs\n",
    "        combined = torch.cat((lstm_out, dense_out), dim=1)\n",
    "        \n",
    "        # Fully connected layers for classification\n",
    "        x = torch.relu(self.fc1(combined))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x.squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea857a87-d398-40db-8550-efc6f1303a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming X_train, X_test, y_train, and y_test are available as numpy arrays\n",
    "# Convert them to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Dataset and DataLoader\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "356b65e6-749c-4cd1-bf3a-d4162ebe2cb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='/workspaces/rumour-detection-gnn/mlruns/36', creation_time=1740494227702, experiment_id='36', last_update_time=1740494227702, lifecycle_stage='active', name='LSTM SMOTE Filter Node 2025-02-22 Transfer Learning sydneysiege', tags={}>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "#mlflow.set_experiment(\"spyder-experiment\")\n",
    "import mlflow.pytorch\n",
    "mlflow.set_experiment(\"LSTM SMOTE Filter Node 2025-02-22 Transfer Learning sydneysiege\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f308407d-6182-4a47-802b-872988c4e486",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score, precision_score\n",
    "\n",
    "# Model, criterion, optimizer initialization (as before)\n",
    "model = RumorDetectionLSTM()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop with loss and recall monitoring\n",
    "epochs = 100  # Adjust as needed\n",
    "train_recall_interval = 50  # Calculate train recall every 10 epochs\n",
    "loss_interval = 50  # Print loss every 10 epochs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c290c7d7-70f7-4880-8021-610cdf99602c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "rumour\n",
      "0    8\n",
      "1    5\n",
      "Name: count, dtype: int64\n",
      "30\n",
      "rumour\n",
      "0    20\n",
      "1    12\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 63\u001b[0m\n\u001b[1;32m     61\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(output, y_batch)\n\u001b[1;32m     62\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 63\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m     epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Print loss every 10 epochs\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/optim/optimizer.py:469\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m cast(Optimizer, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    468\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 469\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecord_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprofile_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# call optimizer step pre hooks\u001b[39;49;00m\n\u001b[1;32m    471\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpre_hook\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_global_optimizer_pre_hooks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer_step_pre_hooks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpre_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/autograd/profiler.py:705\u001b[0m, in \u001b[0;36mrecord_function.__exit__\u001b[0;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting():\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mDisableTorchFunctionSubclass():\n\u001b[0;32m--> 705\u001b[0m         \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_record_function_exit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_RecordFunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    707\u001b[0m     torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39m_record_function_exit(record)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/_ops.py:903\u001b[0m, in \u001b[0;36mTorchBindOpOverload.__call__\u001b[0;34m(self_, *args, **kwargs)\u001b[0m\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m self_\u001b[38;5;241m.\u001b[39m_register_as_effectful_op_temporarily():\n\u001b[1;32m    900\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m self_\u001b[38;5;241m.\u001b[39m_dispatch_in_python(\n\u001b[1;32m    901\u001b[0m             args, kwargs, self_\u001b[38;5;241m.\u001b[39m_fallthrough_keys()\n\u001b[1;32m    902\u001b[0m         )\n\u001b[0;32m--> 903\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mself_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for time_cut in range(15,24*3*60,15):\n",
    "    print(time_cut)\n",
    "    \n",
    "    train_dataset = 'charlie_hebdo'\n",
    "    test_dataset = 'sydneysiege'\n",
    "    processor = Load_Rumours_Dataset_filtering_since_first_post_Transfer_Learning(train_dataset,\\\n",
    "               test_dataset, time_cut=time_cut,test_size=0.7)\n",
    "    \n",
    "    processor.load_data()\n",
    "    processor.process_data()\n",
    "    train,test = processor.get_final_dataframes()\n",
    "\n",
    "    X_train  = train.drop(columns=['rumour'])\n",
    "    X_train = np.hstack([X_train.drop(columns=['embeddings_avg']).values, np.array(pd.DataFrame(X_train.embeddings_avg.tolist()))])\n",
    "    \n",
    "    \n",
    "    X_test  = test.drop(columns=['rumour'])\n",
    "    X_test = np.hstack([X_test.drop(columns=['embeddings_avg']).values, np.array(pd.DataFrame(X_test.embeddings_avg.tolist()))])\n",
    "    \n",
    "    #X = np.hstack([X.drop(columns=['embeddings_avg']).values, np.array(pd.DataFrame(X.embeddings_avg.tolist()))])\n",
    "    y_train =train['rumour']\n",
    "    y_test =test['rumour']\n",
    "\n",
    "    # Apply SMOTE to the features without embeddings\n",
    "    smote = SMOTE(random_state=42,sampling_strategy='minority')\n",
    "    X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "    # Assuming X_train, X_test, y_train, and y_test are available as numpy arrays\n",
    "    # Convert them to PyTorch tensors\n",
    "    X_train = torch.tensor(X_resampled, dtype=torch.float32)\n",
    "    y_train = torch.tensor(y_resampled, dtype=torch.float32)\n",
    "    X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "    \n",
    "    # Dataset and DataLoader\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    test_dataset = TensorDataset(X_test, y_test)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "    \n",
    "\n",
    "    with mlflow.start_run():\n",
    "\n",
    "        # Model, criterion, optimizer initialization (as before)\n",
    "        model = RumorDetectionLSTM()\n",
    "        criterion = nn.BCELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        \n",
    "        # Training loop with loss and recall monitoring\n",
    "        epochs = 200  # Adjust as needed\n",
    "        train_recall_interval = 50  # Calculate train recall every 10 epochs\n",
    "        loss_interval = 50  # Print loss every 10 epochs\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            epoch_loss = 0\n",
    "            for X_batch, y_batch in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                output = model(X_batch)\n",
    "                loss = criterion(output, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                epoch_loss += loss.item()\n",
    "        \n",
    "            # Print loss every 10 epochs\n",
    "            if (epoch + 1) % loss_interval == 0:\n",
    "                model.eval()\n",
    "                train_preds = []\n",
    "                train_labels = []\n",
    "                with torch.no_grad():\n",
    "                    for X_batch, y_batch in train_loader:\n",
    "                        output = model(X_batch)\n",
    "                        preds = (output >= 0.5).int()  # Binarize predictions\n",
    "                        train_preds.extend(preds.tolist())\n",
    "                        train_labels.extend(y_batch.tolist())\n",
    "                \n",
    "                train_recall = recall_score(train_labels, train_preds)\n",
    "                \n",
    "                \n",
    "            \n",
    "        \n",
    "        \n",
    "        # Final evaluation on test set with recall and precision\n",
    "        model.eval()\n",
    "        test_preds = []\n",
    "        test_labels = []\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in test_loader:\n",
    "                output = model(X_batch)\n",
    "                preds = (output >= 0.5).int()  # Binarize predictions\n",
    "                test_preds.extend(preds.tolist())\n",
    "                test_labels.extend(y_batch.tolist())\n",
    "        \n",
    "        # Calculate final test recall and precision\n",
    "        test_recall = recall_score(test_labels, test_preds)\n",
    "        test_precision = precision_score(test_labels, test_preds)\n",
    "\n",
    "        mlflow.log_metric(\"test_precision\",  test_precision)\n",
    "        mlflow.log_metric(\"test_recall\",  test_recall)\n",
    "        \n",
    "                    \n",
    "        mlflow.log_param(\"learning_rate\", 0.001)\n",
    "        mlflow.log_param(\"epochs\", 100)\n",
    "        mlflow.log_metric(\"time_cut\", time_cut)\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
